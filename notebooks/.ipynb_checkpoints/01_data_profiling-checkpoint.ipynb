{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data Profiling: Fraud Detection Analysis\n",
    "\n",
    "**Objective:** Understand fraud distribution and risks in credit card transactions\n",
    "\n",
    "**Key Questions:**\n",
    "- What is the class imbalance ratio?\n",
    "- What is the fraud rate?\n",
    "- Are there potential data leakage features?\n",
    "- What are the characteristics of fraudulent vs legitimate transactions?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Add src to path for imports\u001b[39;00m\n\u001b[32m      5\u001b[39m sys.path.append(os.path.abspath(\u001b[33m'\u001b[39m\u001b[33m../src\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Import our data loading module\n",
    "from data.load_data import load_raw_data, get_data_info, print_data_summary\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 4)\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úì Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = load_raw_data()\n",
    "\n",
    "# Display basic info\n",
    "print_data_summary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types and memory usage\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Class Imbalance Analysis\n",
    "\n",
    "Understanding the severity of class imbalance is critical for fraud detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class distribution\n",
    "class_counts = df['Class'].value_counts()\n",
    "class_percentages = df['Class'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Class Distribution:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Legitimate (0): {class_counts[0]:,} ({class_percentages[0]:.4f}%)\")\n",
    "print(f\"Fraud (1):      {class_counts[1]:,} ({class_percentages[1]:.4f}%)\")\n",
    "print(f\"\\nImbalance Ratio: 1:{class_counts[0]/class_counts[1]:.2f}\")\n",
    "print(f\"Fraud Rate: {class_percentages[1]:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class imbalance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "class_counts.plot(kind='bar', ax=axes[0], color=['#2ecc71', '#e74c3c'])\n",
    "axes[0].set_title('Class Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Class (0=Legitimate, 1=Fraud)', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_xticklabels(['Legitimate', 'Fraud'], rotation=0)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add count labels on bars\n",
    "for i, v in enumerate(class_counts):\n",
    "    axes[0].text(i, v + 5000, f'{v:,}', ha='center', fontweight='bold')\n",
    "\n",
    "# Pie chart\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "axes[1].pie(class_counts, labels=['Legitimate', 'Fraud'], autopct='%1.4f%%',\n",
    "            startangle=90, colors=colors, explode=(0, 0.1))\n",
    "axes[1].set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  SEVERE CLASS IMBALANCE DETECTED\")\n",
    "print(f\"This will require special handling (SMOTE, class weights, etc.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fraud Rate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fraud statistics\n",
    "total_transactions = len(df)\n",
    "fraud_transactions = df['Class'].sum()\n",
    "fraud_rate = (fraud_transactions / total_transactions) * 100\n",
    "\n",
    "print(\"Fraud Rate Analysis:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Transactions: {total_transactions:,}\")\n",
    "print(f\"Fraudulent Transactions: {fraud_transactions:,}\")\n",
    "print(f\"Fraud Rate: {fraud_rate:.4f}%\")\n",
    "print(f\"\\nThis means approximately {int(1/fraud_rate*100)} out of every 10,000 transactions is fraudulent\")\n",
    "print(f\"\\nüí° Business Impact:\")\n",
    "print(f\"   - Accuracy is NOT a good metric (99.8% by predicting all legitimate)\")\n",
    "print(f\"   - Need to focus on Precision, Recall, F1-Score, and AUC-ROC\")\n",
    "print(f\"   - False Negatives (missed fraud) are costly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Analysis\n",
    "\n",
    "Examine the features to understand their distributions and identify potential leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing Values:\")\n",
    "print(\"=\"*50)\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"‚úì No missing values detected!\")\n",
    "else:\n",
    "    print(missing_values[missing_values > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Duplicate Rows: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    print(f\"‚ö†Ô∏è  Warning: {duplicates} duplicate rows found\")\n",
    "else:\n",
    "    print(\"‚úì No duplicate rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Time Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Time feature\n",
    "print(\"Time Feature Statistics:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Min Time: {df['Time'].min()}\")\n",
    "print(f\"Max Time: {df['Time'].max()}\")\n",
    "print(f\"Time Range: {df['Time'].max() - df['Time'].min()} seconds\")\n",
    "print(f\"Time Range: {(df['Time'].max() - df['Time'].min()) / 3600:.2f} hours\")\n",
    "print(f\"Time Range: {(df['Time'].max() - df['Time'].min()) / 86400:.2f} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraud distribution over time\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Convert time to hours for better visualization\n",
    "df['Time_Hours'] = df['Time'] / 3600\n",
    "\n",
    "# Plot 1: All transactions over time\n",
    "axes[0].scatter(df[df['Class']==0]['Time_Hours'], \n",
    "                df[df['Class']==0].index, \n",
    "                alpha=0.3, s=1, label='Legitimate', color='blue')\n",
    "axes[0].scatter(df[df['Class']==1]['Time_Hours'], \n",
    "                df[df['Class']==1].index, \n",
    "                alpha=0.8, s=10, label='Fraud', color='red')\n",
    "axes[0].set_xlabel('Time (hours)', fontsize=12)\n",
    "axes[0].set_ylabel('Transaction Index', fontsize=12)\n",
    "axes[0].set_title('Transaction Distribution Over Time', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Fraud rate over time bins\n",
    "time_bins = pd.cut(df['Time_Hours'], bins=20)\n",
    "fraud_rate_by_time = df.groupby(time_bins)['Class'].agg(['sum', 'count'])\n",
    "fraud_rate_by_time['fraud_rate'] = (fraud_rate_by_time['sum'] / fraud_rate_by_time['count']) * 100\n",
    "\n",
    "axes[1].plot(range(len(fraud_rate_by_time)), fraud_rate_by_time['fraud_rate'], \n",
    "             marker='o', linewidth=2, markersize=6, color='red')\n",
    "axes[1].set_xlabel('Time Bin', fontsize=12)\n",
    "axes[1].set_ylabel('Fraud Rate (%)', fontsize=12)\n",
    "axes[1].set_title('Fraud Rate Over Time Bins', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Insight: Check if fraud rate varies significantly over time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Amount Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount statistics by class\n",
    "print(\"Amount Statistics by Class:\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nLegitimate Transactions:\")\n",
    "print(df[df['Class']==0]['Amount'].describe())\n",
    "print(\"\\nFraudulent Transactions:\")\n",
    "print(df[df['Class']==1]['Amount'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Amount distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Amount distribution for legitimate\n",
    "axes[0, 0].hist(df[df['Class']==0]['Amount'], bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Amount', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title('Legitimate Transactions - Amount Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Amount distribution for fraud\n",
    "axes[0, 1].hist(df[df['Class']==1]['Amount'], bins=50, color='red', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Amount', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 1].set_title('Fraudulent Transactions - Amount Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Box plot comparison\n",
    "df.boxplot(column='Amount', by='Class', ax=axes[1, 0])\n",
    "axes[1, 0].set_xlabel('Class (0=Legitimate, 1=Fraud)', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Amount', fontsize=12)\n",
    "axes[1, 0].set_title('Amount Distribution by Class', fontsize=12, fontweight='bold')\n",
    "plt.sca(axes[1, 0])\n",
    "plt.xticks([1, 2], ['Legitimate', 'Fraud'])\n",
    "\n",
    "# Plot 4: Log-scale comparison\n",
    "axes[1, 1].hist(df[df['Class']==0]['Amount'], bins=50, alpha=0.5, label='Legitimate', color='blue')\n",
    "axes[1, 1].hist(df[df['Class']==1]['Amount'], bins=50, alpha=0.5, label='Fraud', color='red')\n",
    "axes[1, 1].set_xlabel('Amount', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Frequency (log scale)', fontsize=12)\n",
    "axes[1, 1].set_yscale('log')\n",
    "axes[1, 1].set_title('Amount Distribution Comparison (Log Scale)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 PCA Features (V1-V28) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get V features\n",
    "v_features = [col for col in df.columns if col.startswith('V')]\n",
    "print(f\"Number of PCA features: {len(v_features)}\")\n",
    "print(f\"Features: {v_features[:5]}... (showing first 5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation with target for each V feature\n",
    "correlations = df[v_features + ['Class']].corr()['Class'].drop('Class').sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 Features Most Correlated with Fraud:\")\n",
    "print(\"=\"*50)\n",
    "print(correlations.head(10))\n",
    "print(\"\\nTop 10 Features Most Negatively Correlated with Fraud:\")\n",
    "print(\"=\"*50)\n",
    "print(correlations.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature correlations with target\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "correlations_sorted = correlations.sort_values()\n",
    "colors = ['red' if x < 0 else 'green' for x in correlations_sorted]\n",
    "\n",
    "correlations_sorted.plot(kind='barh', ax=ax, color=colors, alpha=0.7)\n",
    "ax.set_xlabel('Correlation with Fraud (Class)', fontsize=12)\n",
    "ax.set_ylabel('Features', fontsize=12)\n",
    "ax.set_title('Feature Correlation with Fraud Class', fontsize=14, fontweight='bold')\n",
    "ax.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Features with high absolute correlation may be most predictive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution of top correlated features\n",
    "top_features = correlations.abs().sort_values(ascending=False).head(6).index.tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, feature in enumerate(top_features):\n",
    "    axes[idx].hist(df[df['Class']==0][feature], bins=50, alpha=0.5, label='Legitimate', color='blue', density=True)\n",
    "    axes[idx].hist(df[df['Class']==1][feature], bins=50, alpha=0.5, label='Fraud', color='red', density=True)\n",
    "    axes[idx].set_xlabel(feature, fontsize=11)\n",
    "    axes[idx].set_ylabel('Density', fontsize=11)\n",
    "    axes[idx].set_title(f'{feature} Distribution (Corr: {correlations[feature]:.3f})', \n",
    "                        fontsize=11, fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Leakage Detection\n",
    "\n",
    "Check for potential data leakage issues that could artificially inflate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Leakage Checks:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check 1: Perfect separation in any feature\n",
    "print(\"\\n1. Checking for perfect separation in features...\")\n",
    "perfect_separation = False\n",
    "for col in df.columns:\n",
    "    if col != 'Class':\n",
    "        fraud_vals = set(df[df['Class']==1][col].unique())\n",
    "        legit_vals = set(df[df['Class']==0][col].unique())\n",
    "        if fraud_vals.isdisjoint(legit_vals):\n",
    "            print(f\"   ‚ö†Ô∏è  WARNING: Perfect separation in {col}\")\n",
    "            perfect_separation = True\n",
    "\n",
    "if not perfect_separation:\n",
    "    print(\"   ‚úì No perfect separation detected\")\n",
    "\n",
    "# Check 2: Suspiciously high correlations\n",
    "print(\"\\n2. Checking for suspiciously high correlations...\")\n",
    "high_corr_threshold = 0.9\n",
    "high_corr_features = correlations[correlations.abs() > high_corr_threshold]\n",
    "if len(high_corr_features) > 0:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: Features with correlation > {high_corr_threshold}:\")\n",
    "    print(high_corr_features)\n",
    "else:\n",
    "    print(f\"   ‚úì No features with correlation > {high_corr_threshold}\")\n",
    "\n",
    "# Check 3: Time-based leakage\n",
    "print(\"\\n3. Checking for time-based leakage...\")\n",
    "print(\"   üí° Time feature represents seconds elapsed - should be safe\")\n",
    "print(\"   üí° However, be cautious about temporal patterns in deployment\")\n",
    "\n",
    "# Check 4: Feature scaling issues\n",
    "print(\"\\n4. Checking feature scales...\")\n",
    "print(f\"   - V features (PCA): Already scaled (mean ‚âà 0, std ‚âà 1)\")\n",
    "print(f\"   - Time: Range = {df['Time'].min():.0f} to {df['Time'].max():.0f}\")\n",
    "print(f\"   - Amount: Range = {df['Amount'].min():.2f} to {df['Amount'].max():.2f}\")\n",
    "print(f\"   ‚ö†Ô∏è  Time and Amount need scaling before modeling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary insights\n",
    "info = get_data_info(df)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY INSIGHTS & RECOMMENDATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä CLASS IMBALANCE:\")\n",
    "print(f\"   - Fraud rate: {info['fraud_percentage']:.4f}%\")\n",
    "print(f\"   - Imbalance ratio: 1:{info['legitimate_count']/info['fraud_count']:.0f}\")\n",
    "print(f\"   - Recommendation: Use SMOTE, class weights, or ensemble methods\")\n",
    "\n",
    "print(\"\\nüìà EVALUATION METRICS:\")\n",
    "print(f\"   - DO NOT use accuracy as primary metric\")\n",
    "print(f\"   - Focus on: Precision, Recall, F1-Score, PR-AUC, ROC-AUC\")\n",
    "print(f\"   - Consider business cost of False Negatives vs False Positives\")\n",
    "\n",
    "print(\"\\nüîß PREPROCESSING NEEDED:\")\n",
    "print(f\"   - Scale 'Time' and 'Amount' features\")\n",
    "print(f\"   - V features are already PCA-transformed and scaled\")\n",
    "print(f\"   - Consider creating time-based features (hour of day, etc.)\")\n",
    "\n",
    "print(\"\\nüéØ MODELING STRATEGY:\")\n",
    "print(f\"   - Use stratified train-test split to maintain class ratio\")\n",
    "print(f\"   - Apply resampling techniques (SMOTE, ADASYN)\")\n",
    "print(f\"   - Try ensemble methods (Random Forest, XGBoost, LightGBM)\")\n",
    "print(f\"   - Use cross-validation with stratification\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  DATA QUALITY:\")\n",
    "print(f\"   - Missing values: {info['missing_values']} ‚úì\")\n",
    "print(f\"   - Duplicate rows: {info['duplicate_rows']} ‚úì\")\n",
    "print(f\"   - No obvious data leakage detected ‚úì\")\n",
    "\n",
    "print(\"\\nüîç FEATURE IMPORTANCE:\")\n",
    "top_3_features = correlations.abs().sort_values(ascending=False).head(3)\n",
    "print(f\"   Top 3 correlated features:\")\n",
    "for feat, corr in top_3_features.items():\n",
    "    print(f\"   - {feat}: {corr:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úì Data profiling complete! Ready for preprocessing and modeling.\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "Based on this analysis, the next steps are:\n",
    "\n",
    "1. **Data Preprocessing** (Notebook 02):\n",
    "   - Scale Time and Amount features\n",
    "   - Create additional time-based features\n",
    "   - Prepare train-test split with stratification\n",
    "\n",
    "2. **Handle Class Imbalance** (Notebook 03):\n",
    "   - Apply SMOTE or ADASYN\n",
    "   - Experiment with class weights\n",
    "   - Compare different resampling strategies\n",
    "\n",
    "3. **Model Training** (Notebook 04):\n",
    "   - Train baseline models\n",
    "   - Evaluate with appropriate metrics\n",
    "   - Optimize hyperparameters\n",
    "\n",
    "4. **Model Evaluation** (Notebook 05):\n",
    "   - Compare model performance\n",
    "   - Analyze confusion matrices\n",
    "   - Calculate business impact metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
