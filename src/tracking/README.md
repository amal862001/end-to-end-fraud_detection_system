# ğŸ”¬ Experiment Tracking with MLflow + DagsHub

This module provides comprehensive experiment tracking for the fraud detection project using **MLflow** with optional **DagsHub** integration.

---

## ğŸ“‹ **What Gets Tracked**

### **1. Parameters**
- Model hyperparameters (max_depth, learning_rate, etc.)
- Data configuration (train/test split, scaling method)
- Preprocessing steps
- Random seeds

### **2. Metrics**
- **PR-AUC** (Precision-Recall AUC) - Primary metric for imbalanced data
- **ROC-AUC** (Receiver Operating Characteristic AUC)
- **Precision, Recall, F1-Score**
- **Confusion Matrix** (TP, FP, TN, FN)
- **Accuracy, FPR, FNR**

### **3. Artifacts**
- **Models** (saved in MLflow format)
- **Plots**:
  - Confusion Matrix
  - Precision-Recall Curve
  - Feature Importance
- **Data**:
  - Feature importance CSV
  - Parameters JSON
  - Metrics summary

### **4. Model Registry**
- Version control for models
- Stage management (Staging, Production)
- Model lineage tracking

---

## ğŸš€ **Quick Start**

### **Option 1: Simple Function API**

```python
from tracking.mlflow_utils import track_experiment

# Track a complete experiment
metrics = track_experiment(
    experiment_name="fraud-detection",
    run_name="lightgbm-v1",
    model=trained_model,
    X_train=X_train,
    y_train=y_train,
    X_test=X_test,
    y_test=y_test,
    feature_names=feature_names,
    params={"max_depth": 9, "learning_rate": 0.05},
    tags={"model_type": "LightGBM", "version": "v1"}
)
```

### **Option 2: Class-Based API (More Control)**

```python
from tracking.mlflow_utils import MLflowTracker

# Initialize tracker
tracker = MLflowTracker(experiment_name="fraud-detection")

# Start a run
tracker.start_run(
    run_name="lightgbm-v1",
    tags={"model_type": "LightGBM"},
    description="LightGBM with default parameters"
)

# Log parameters
tracker.log_params({
    "max_depth": 9,
    "learning_rate": 0.05,
    "n_estimators": 100
})

# Log metrics
tracker.log_metrics({
    "pr_auc": 0.8689,
    "roc_auc": 0.9684,
    "precision": 0.88,
    "recall": 0.84
})

# Log complete evaluation (metrics + plots + model)
tracker.log_complete_evaluation(
    model=trained_model,
    X_test=X_test,
    y_test=y_test,
    feature_names=feature_names,
    model_name="lightgbm"
)

# End run
tracker.end_run()
```

### **Option 3: Context Manager**

```python
from tracking.mlflow_utils import MLflowTracker

with MLflowTracker(experiment_name="fraud-detection") as tracker:
    tracker.start_run(run_name="experiment-1")
    tracker.log_params(params)
    tracker.log_metrics(metrics)
    tracker.log_model(model, "model")
    # Automatically ends run when exiting context
```

---

## ğŸ“Š **Track All Models**

Run the provided script to track all your trained models:

```bash
python src/tracking/track_experiments.py
```

This will track:
- âœ… Random Forest (baseline)
- âœ… XGBoost
- âœ… LightGBM (default) - **BEST MODEL**
- âœ… LightGBM (optimized)

---

## ğŸŒ **DagsHub Integration**

### **Setup DagsHub (Optional)**

1. **Create a DagsHub account**: https://dagshub.com
2. **Create a repository**: `fraud-detection`
3. **Set environment variables**:

```bash
# Windows (PowerShell)
$env:DAGSHUB_USER = "your-username"
$env:DAGSHUB_TOKEN = "your-token"

# Linux/Mac
export DAGSHUB_USER="your-username"
export DAGSHUB_TOKEN="your-token"
```

4. **Initialize tracker with DagsHub**:

```python
tracker = MLflowTracker(
    experiment_name="fraud-detection",
    dagshub_repo="fraud-detection",
    dagshub_user="your-username"
)
```

5. **View experiments**: https://dagshub.com/your-username/fraud-detection/experiments

---

## ğŸ–¥ï¸ **View Experiments Locally**

### **Start MLflow UI**

```bash
mlflow ui
```

Then open: http://localhost:5000

### **What You'll See**

- **Experiments**: All your experiment runs
- **Metrics**: Compare PR-AUC, ROC-AUC, etc. across runs
- **Parameters**: See which hyperparameters were used
- **Artifacts**: Download models, plots, and data
- **Charts**: Visualize metric trends

---

## ğŸ“ **File Structure**

```
src/tracking/
â”œâ”€â”€ mlflow_utils.py          # Main tracking utilities
â”œâ”€â”€ track_experiments.py     # Script to track all models
â””â”€â”€ README.md               # This file

mlruns/                      # Local MLflow tracking data
â”œâ”€â”€ 0/                      # Experiment ID
â”‚   â”œâ”€â”€ meta.yaml
â”‚   â””â”€â”€ <run-id>/          # Individual runs
â”‚       â”œâ”€â”€ artifacts/     # Models, plots, data
â”‚       â”œâ”€â”€ metrics/       # Metric values
â”‚       â”œâ”€â”€ params/        # Parameter values
â”‚       â””â”€â”€ tags/          # Tags and metadata
```

---

## ğŸ¯ **Best Practices**

### **1. Naming Conventions**

```python
# Good run names
run_name="lightgbm-default-v1"
run_name="xgboost-tuned-2026-01-15"
run_name="rf-baseline"

# Bad run names
run_name="test"
run_name="model1"
```

### **2. Use Tags**

```python
tags={
    "model_type": "LightGBM",
    "version": "v1",
    "best": "true",
    "optimizer": "Optuna"
}
```

### **3. Add Descriptions**

```python
description="LightGBM with default parameters. Best model with PR-AUC=0.8689"
```

### **4. Log Everything**

- âœ… All hyperparameters
- âœ… Data statistics (train size, fraud ratio)
- âœ… Preprocessing steps
- âœ… Random seeds
- âœ… Training time

---

## ğŸ“ˆ **Example Output**

```
============================================================
MLFLOW RUN STARTED
============================================================
Run ID: a1b2c3d4e5f6
Run Name: lightgbm-default
Description: LightGBM with default parameters - BEST MODEL
============================================================

âœ“ Logged 8 parameters
âœ“ Logged 12 metrics
âœ“ Logged confusion matrix
âœ“ Logged PR curve
âœ“ Logged feature importance (plot + CSV)
âœ“ Logged model: lightgbm_default

============================================================
âœ“ COMPLETE EVALUATION LOGGED
============================================================
```

---

## ğŸ”§ **Troubleshooting**

### **Issue: DagsHub connection fails**

**Solution**: Check credentials and network connection
```python
# Verify environment variables
import os
print(os.getenv('DAGSHUB_USER'))
print(os.getenv('DAGSHUB_TOKEN'))
```

### **Issue: MLflow UI shows no experiments**

**Solution**: Check tracking URI
```python
import mlflow
print(mlflow.get_tracking_uri())
# Should be: file:./mlruns or DagsHub URL
```

### **Issue: Model logging fails**

**Solution**: Ensure model type is supported
```python
# Supported: sklearn, xgboost, lightgbm
# For custom models, use mlflow.pyfunc
```

---

## ğŸ“š **Resources**

- **MLflow Documentation**: https://mlflow.org/docs/latest/index.html
- **DagsHub Documentation**: https://dagshub.com/docs
- **MLflow Tracking**: https://mlflow.org/docs/latest/tracking.html
- **Model Registry**: https://mlflow.org/docs/latest/model-registry.html

---

## âœ¨ **Summary**

This tracking module provides:
- âœ… **Complete experiment tracking** (params, metrics, artifacts)
- âœ… **Easy-to-use API** (function or class-based)
- âœ… **DagsHub integration** (optional remote tracking)
- âœ… **Model registry** (versioning and deployment)
- âœ… **Visualization** (MLflow UI or DagsHub)

**Perfect for:**
- ğŸ¯ Comparing different models
- ğŸ“Š Tracking hyperparameter tuning
- ğŸš€ Model versioning and deployment
- ğŸ“ Documenting experiments for portfolio/interviews

---

**Happy Tracking!** ğŸš€

